{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Set up download directory\n",
    "ACCOUNT_URL = \"https://saqdiveassignments.blob.core.windows.net/\"\n",
    "CONTAINER_NAME = \"dataengineerfiles\"\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), \"data\")\n",
    "if not os.path.exists(DATA_DIR_PATH):\n",
    "    os.makedirs(DATA_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AzureBlobOperations:\n",
    "    def __init__(self, storage_url: str, credentials=None) -> None:\n",
    "        self.blob_service_client = BlobServiceClient(storage_url, credential=credentials)\n",
    "    \n",
    "    def download_blob_to_file(self, container_name: str, blob_name: str):\n",
    "        blob_client = self.blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "        with open(file=os.path.join(DATA_DIR_PATH, blob_name), mode=\"wb\") as sample_blob:\n",
    "            download_stream = blob_client.download_blob()\n",
    "            sample_blob.write(download_stream.readall())\n",
    "\n",
    "    def download_all_blobs_to_dir(self, container_name: str):\n",
    "        \"\"\"Downloads all blobs contained in a container on Azure Storage.\n",
    "        \n",
    "        Based on the download blob function from Azure Documentation.\n",
    "\n",
    "        Args:\n",
    "            container_name: Name of the storage container containing the blob.\n",
    "        \"\"\"\n",
    "        container_client = self.blob_service_client.get_container_client(container=container_name)\n",
    "        for blob_name in container_client.list_blob_names():\n",
    "            blob_client = container_client.get_blob_client(blob=blob_name)\n",
    "            with open(file=os.path.join(DATA_DIR_PATH, blob_name), mode=\"wb\") as sample_blob:\n",
    "                download_stream = blob_client.download_blob()\n",
    "                sample_blob.write(download_stream.readall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_op = AzureBlobOperations(storage_url=ACCOUNT_URL, credentials=None)\n",
    "container_client = azure_op.blob_service_client.get_container_client(container=CONTAINER_NAME)\n",
    "for blob in container_client.list_blobs():\n",
    "    print(f\"Name: {blob.name}, Size: {blob.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure_op.download_all_blobs_to_dir(CONTAINER_NAME)\n",
    "azure_op.download_blob_to_file(CONTAINER_NAME, \"titles.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as polars/pandas dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Engineering\n",
    "To be able to scale the data engineering steps, it is necessary to identify what steps are required. To do this the following needs to be investigated:\n",
    "\n",
    "### 1. Visualize raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dataframe info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean data\n",
    "Bring the data into Best-Practice format for easier manipulation from data scientists down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "# Treat null values sensibly\n",
    "# One-hot encode categorical columns\n",
    "# Set consistent schema by converting types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Aggregate/Transform data\n",
    "Reduce redundant information and transform data into the most useful and expressive representation considering analysis and ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write results\n",
    "After refining the input data, it needs to be made accessible to the data scientists. The optimal methods depends strongly on their tools and use-case. \n",
    "For this exercise i will assume they want to generate a dashboard for monitoring (repeating the same analysis), creating estimators based on the data (ML) or both (MLOps).\n",
    "Therefore it makes sense to host the data in a ready available application like Google BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to BigQuery\n",
    "\n",
    "# Upload to project QDive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
